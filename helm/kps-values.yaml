nameOverride: "kps"
namespaceOverride: "monitoring"
crds:
  enabled: false
alertmanager:
  config:
    route:
      group_by: ['job']
      receiver: 'pushover'
      routes:
        - matchers:
            - alertname = "Watchdog"
          receiver: 'null'
        - matchers:
            - alertname = "NodeDiskIOSaturation"
          receiver: 'null'

    receivers:
      - name: 'pushover'
        pushover_configs:
          - user_key_file: /etc/alertmanager/secrets/pushover-secret/token
            token_file: /etc/alertmanager/secrets/pushover-secret/homelab-apikey
            title: "Homelab Alert"
            message: |-
              ðŸ”” {{ .CommonLabels.alertname }} [{{ .CommonLabels.severity }}]
              ðŸ”¹ Instance: {{ .CommonLabels.instance }}
              ðŸ”¹ Job: {{ .CommonLabels.job }}
              {{- if .CommonLabels.namespace }}
              ðŸ”¹ Namespace: {{ .CommonLabels.namespace }}
              {{- end }}
              {{- if .CommonAnnotations.description }}
              ðŸ”¸ {{ .CommonAnnotations.description }}
              {{- else if .CommonAnnotations.summary }}
              ðŸ”¸ {{ .CommonAnnotations.summary }}
              {{- end }}
            priority: 0
            sound: bugle
            url: "https://grafana.sith.network"
            url_title: "Grafana"
            send_resolved: true

  templateFiles:
    template_1.tmpl: |-
      {{ define "__alertmanager" }}Alertmanager{{ end }}
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      external-dns.alpha.kubernetes.io/target: "alertmanager.sith.network"
    hosts:
      - alertmanager.sith.network
    paths:
      - /
    tls:
      - secretName: wildcard-sith-network
        hosts:
          - alertmanager.sith.network
  alertmanagerSpec:
    secrets:
      - pushover-secret

grafana:
  namespaceOverride: "monitoring"
  defaultDashboardsEnabled: false
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      external-dns.alpha.kubernetes.io/target: "grafana.sith.network"
    hosts:
      - grafana.sith.network
    path: /
    tls:
      - secretName: wildcard-sith-network
        hosts:
          - grafana.sith.network
  persistence:
    type: pvc
    enabled: true
    accessModes:
      - ReadWriteOnce
    size: 20Gi
    finalizers:
      - kubernetes.io/pvc-protection
  grafana.ini:
    enable_gzip: true
    auth.anonymous:
      enabled: true
    auth.ldap:
      enabled: true
      config_file: /etc/grafana/ldap.toml
    dataproxy:
      timeout: 300
  ldap:
    enabled: true
    existingSecret: "grafana-ldap"
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'grafana-dashboards-kubernetes'
        orgId: 1
        folder: 'Kubernetes'
        type: file
        disableDeletion: true
        editable: true
        options:
          path: /var/lib/grafana/dashboards/grafana-dashboards-kubernetes
  dashboards:
    grafana-dashboards-kubernetes:
      k8s-system-api-server:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-api-server.json
        token: ''
      k8s-system-coredns:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-system-coredns.json
        token: ''
      k8s-views-global:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-global.json
        token: ''
      k8s-views-namespaces:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-namespaces.json
        token: ''
      k8s-views-nodes:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-nodes.json
        token: ''
      k8s-views-pods:
        url: https://raw.githubusercontent.com/dotdc/grafana-dashboards-kubernetes/master/dashboards/k8s-views-pods.json
        token: ''
kubeControllerManager:
  service:
    selector:
      k8s-app: kube-controller-manager
kubeEtcd:
  enabled: true
  endpoints:
    - 192.168.0.216
  service:
    enabled: true
    port: 2381
    targetPort: 2381
    selector: {}
  serviceMonitor:
    enabled: true
    scheme: http
    insecureSkipVerify: true
    interval: 30s
    scrapeTimeout: 10s
    tlsConfig:
      insecureSkipVerify: true

kubeScheduler:
  service:
    selector:
      k8s-app: kube-scheduler
kube-state-metrics:
  namespaceOverride: "monitoring"
prometheus-node-exporter:
  namespaceOverride: "monitoring"
prometheus:
  ingress:
    enabled: true
    annotations:
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      external-dns.alpha.kubernetes.io/target: "prometheus.sith.network"
    hosts:
      - prometheus.sith.network
    paths:
    - /
    tls:
      - secretName: wildcard-sith-network
        hosts:
          - prometheus.sith.network
  prometheusSpec:
    additionalScrapeConfigs:
      - job_name: minio-job
        metrics_path: /minio/metrics/v3
        scheme: http
        static_configs:
        - targets: ['s3.sith.network:9000']
      - job_name: minio-job-bucket
        metrics_path: /minio/v2/metrics/bucket
        scheme: http
        static_configs:
        - targets: ['s3.sith.network:9000']
    storageSpec:
      volumeClaimTemplate:
        metadata:
          name: prometheus-data
        spec:
          storageClassName: nfs-client
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi
